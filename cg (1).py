# -*- coding: utf-8 -*-
"""CG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vzq8mTBJaKP4kMRC_XVdZAxGQrOlZJcg

#Step 1: Setup & Install Dependencies
"""

!pip install faiss-cpu pandas datasets torch transformers accelerate sentence-transformers

from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import faiss
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import ast
import re
from typing import List, Dict, Optional, Tuple
import logging
import google.generativeai as genai
from google.colab import userdata
import time
from tenacity import retry, stop_after_attempt, wait_exponential

import google.generativeai as genai
import os # Import the os module

try:
    # Replace this line with your actual API key
    GOOGLE_API_KEY = "AIzaSyB2_w02Fn0TzwB24Ksh6gi4e6cRqrgSLCg"
    genai.configure(api_key=GOOGLE_API_KEY)
except Exception as e:
    print(f"Error configuring Gemini: {e}")

class EmbeddingPipeline:
    """Enhanced embedding generation and FAISS index creation"""
    def __init__(self, model_name="all-mpnet-base-v2"):
        self.model = SentenceTransformer(model_name)
        self.prompts = None
        self.embeddings = None

    def create_embeddings(self, dataset_path="/content/human-eval.jsonl"):
        """Load dataset and create embeddings with preprocessing"""
        try:
            dataset = pd.read_json(dataset_path, lines=True)

            # Preprocess prompts - combine prompt and canonical_solution if available
            self.prompts = []
            for _, row in dataset.iterrows():
                # Create richer context by including both prompt and solution pattern
                prompt_text = row["prompt"]

                # Extract function signature and docstring for better matching
                if "def " in prompt_text and '"""' in prompt_text:
                    # Extract the docstring for semantic understanding
                    docstring_match = re.search(r'"""(.*?)"""', prompt_text, re.DOTALL)
                    if docstring_match:
                        docstring = docstring_match.group(1).strip()
                        prompt_text = f"{prompt_text}\nDescription: {docstring}"

                self.prompts.append(prompt_text)

            # Create embeddings
            self.embeddings = self.model.encode(self.prompts, show_progress_bar=True)
            self.embeddings = np.array(self.embeddings).astype("float32")

            print(f"Created embeddings for {len(self.prompts)} examples")
            return self.embeddings

        except Exception as e:
            logging.error(f"Error creating embeddings: {e}")
            raise

    def create_faiss_index(self, index_path="humaneval_embeddings.faiss"):
        """Create and save FAISS index with better indexing strategy"""
        if self.embeddings is None:
            raise ValueError("Embeddings not created yet. Call create_embeddings first.")

        dimension = self.embeddings.shape[1]

        # Use IVFFlat for better performance with larger datasets
        if len(self.embeddings) > 1000:
            nlist = min(100, len(self.embeddings) // 10)  # Number of clusters
            quantizer = faiss.IndexFlatL2(dimension)
            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            index.train(self.embeddings)
        else:
            index = faiss.IndexFlatL2(dimension)

        index.add(self.embeddings)
        faiss.write_index(index, index_path)
        return index

class RetrievalEngine:
    """Enhanced similarity search with filtering and ranking"""
    def __init__(self, index_path: str, prompts: List[str]):
        self.index = faiss.read_index(index_path)
        self.prompts = prompts
        self.embedder = SentenceTransformer("all-mpnet-base-v2")

    def retrieve(self, query: str, top_k: int = 5, similarity_threshold: float = 0.3) -> List[Dict]:
        """Retrieve and rank similar code examples with filtering"""
        try:
            query_embedding = np.array(self.embedder.encode([query])).astype("float32")

            # Search for more candidates than needed
            search_k = min(top_k * 3, len(self.prompts))
            distances, indices = self.index.search(query_embedding, search_k)

            # Filter and rank results
            filtered_results = []
            for dist, idx in zip(distances[0], indices[0]):
                if idx != -1:  # Valid index
                    # Convert L2 distance to similarity score (lower distance = higher similarity)
                    similarity = 1 / (1 + dist)

                    if similarity > similarity_threshold:
                        filtered_results.append({
                            'prompt': self.prompts[idx],
                            'code': self.prompts[idx],  # Add code key for compatibility
                            'similarity': similarity,
                            'distance': dist
                        })

            # Sort by similarity and return top_k
            filtered_results.sort(key=lambda x: x['similarity'], reverse=True)
            return filtered_results[:top_k]

        except Exception as e:
            logging.error(f"Error in retrieval: {e}")
            return []

    def extract_function_info(self, code: str) -> Dict:
        """Extract function metadata for better matching"""
        try:
            # Parse the AST to get function information
            tree = ast.parse(code)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    return {
                        'name': node.name,
                        'args': [arg.arg for arg in node.args.args],
                        'docstring': ast.get_docstring(node) or ''
                    }
        except:
            pass
        return {}

class LocalCodeGenerator:
    def __init__(self, model_name="Salesforce/codegen-350M-mono"):
        print(f"Loading model: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        print("Model loaded successfully!")

    def generate(self, prompt: str, retrieved_examples: list) -> dict:
        # Format the prompt as a comment, which the model is trained on
        formatted_prompt = f"# {prompt}\n"

        # Add retrieved examples to the prompt if they exist
        if retrieved_examples:
            examples_text = "\n".join([f"# Example:\n{ex['code']}" for ex in retrieved_examples])
            formatted_prompt = f"{examples_text}\n{formatted_prompt}"

        # Tokenize the prompt
        input_ids = self.tokenizer(formatted_prompt, return_tensors="pt").input_ids

        try:
            # Generate new tokens
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=256,
                    pad_token_id=self.tokenizer.eos_token_id,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.95,
                    repetition_penalty=1.1
                )

            # Decode only the newly generated part of the output
            generated_code = self.tokenizer.decode(
                outputs[0, input_ids.shape[-1]:],
                skip_special_tokens=True
            ).strip()

        except Exception as e:
            print(f"Error during model generation: {e}")
            generated_code = ""

        return {
            'generated_code': generated_code,  # Fixed key name
            'raw_output': generated_code,
            'full_prompt': formatted_prompt
        }

class EnhancedLocalCodeGenerator:
    def __init__(self, model_name="Salesforce/codegen-350M-mono"):
        print(f"Loading model: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        print("Model loaded successfully!")

    def extract_function_signature(self, prompt):
        """Extract function name from natural language prompt"""
        # Simple heuristics to suggest function names
        if "below zero" in prompt.lower():
            return "is_below_zero"
        elif "positive" in prompt.lower():
            return "is_positive"
        elif "even" in prompt.lower():
            return "is_even"
        elif "prime" in prompt.lower():
            return "is_prime"
        else:
            return "solution"

    def format_examples(self, retrieved_examples):
        """Format retrieved examples for better context"""
        if not retrieved_examples:
            return ""

        examples_text = "# Here are some similar examples:\n"
        for i, example in enumerate(retrieved_examples[:2]):  # Use top 2 examples
            # Extract just the function definition part
            code_lines = example['code'].split('\n')
            func_lines = []
            in_function = False

            for line in code_lines:
                if line.strip().startswith('def '):
                    in_function = True
                    func_lines.append(line)
                elif in_function:
                    if line.strip() == '' or line.startswith(' ') or line.startswith('\t'):
                        func_lines.append(line)
                    else:
                        break

            if func_lines:
                examples_text += f"# Example {i+1}:\n"
                examples_text += '\n'.join(func_lines[:10])  # Limit lines
                examples_text += "\n\n"

        return examples_text

    def generate(self, prompt: str, retrieved_examples: list) -> dict:
        # Suggest a function name based on the prompt
        suggested_func_name = self.extract_function_signature(prompt)

        # Format examples
        examples_context = self.format_examples(retrieved_examples)

        # Create a more structured prompt
        formatted_prompt = f"""{examples_context}# Task: {prompt}
# Please write a Python function called '{suggested_func_name}' that solves this task:

def {suggested_func_name}("""

        # Tokenize the prompt
        input_ids = self.tokenizer(formatted_prompt, return_tensors="pt").input_ids

        try:
            # Generate new tokens with better parameters
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=200,
                    pad_token_id=self.tokenizer.eos_token_id,
                    do_sample=True,
                    temperature=0.4,  # Lower temperature for more focused generation
                    top_p=0.9,
                    repetition_penalty=1.2,
                    num_return_sequences=1,
                    early_stopping=True
                )

            # Decode the full output
            full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Extract just the generated part
            generated_part = full_output[len(formatted_prompt):].strip()

            # Complete the function definition
            complete_function = f"def {suggested_func_name}(" + generated_part

            # Clean up the output
            complete_function = self.clean_generated_code(complete_function)

        except Exception as e:
            print(f"Error during model generation: {e}")
            complete_function = f"""def {suggested_func_name}(x):
    # Generated function for: {prompt}
    pass"""

        return {
            'generated_code': complete_function,
            'raw_output': generated_part if 'generated_part' in locals() else '',
            'full_prompt': formatted_prompt,
            'function_name': suggested_func_name
        }

    def clean_generated_code(self, code):
        """Clean and format the generated code"""
        lines = code.split('\n')
        cleaned_lines = []

        for line in lines:
            # Stop at obvious end markers
            if any(marker in line.lower() for marker in ['# example', '# task', '# note', 'print(']):
                break
            cleaned_lines.append(line)

        # Remove trailing empty lines
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()

        return '\n'.join(cleaned_lines)

class RAGCodeGenerator:
    """Enhanced end-to-end RAG pipeline"""
    def __init__(self, dataset_path: str = "/content/human-eval.jsonl"):
        # Initialize components with error handling
        try:
            print("Initializing RAG Code Generator...")
            self.embedding_pipeline = EmbeddingPipeline()
            self.embeddings = self.embedding_pipeline.create_embeddings(dataset_path)
            self.index = self.embedding_pipeline.create_faiss_index()

            self.retriever = RetrievalEngine(
                "humaneval_embeddings.faiss",
                self.embedding_pipeline.prompts
            )

            self.generator = EnhancedLocalCodeGenerator()
            print("RAG system initialized successfully!")

        except Exception as e:
            logging.error(f"Error initializing RAG system: {e}")
            raise

    def generate_code(self, user_prompt: str, top_k: int = 3, similarity_threshold: float = 0.3) -> Dict:
        """Enhanced pipeline execution with comprehensive results"""
        try:
            print(f"Generating code for prompt: {user_prompt}")

            # Retrieve similar examples
            similar_examples = self.retriever.retrieve(
                user_prompt,
                top_k=top_k,
                similarity_threshold=similarity_threshold
            )

            print(f"Found {len(similar_examples)} similar examples")

            # Generate code
            generation_result = self.generator.generate(user_prompt, similar_examples)

            return {
                "prompt": user_prompt,
                "retrieved_examples": similar_examples,
                "generated_code": generation_result['generated_code'],  # Fixed key access
                "raw_output": generation_result['raw_output'],
                "retrieval_count": len(similar_examples)
            }

        except Exception as e:
            logging.error(f"Error in code generation pipeline: {e}")
            return {
                "prompt": user_prompt,
                "retrieved_examples": [],
                "generated_code": "",
                "raw_output": "",
                "retrieval_count": 0,
                "error": str(e)
            }

def validate_and_execute(generated_code, function_name, test_cases):
    """
    This function safely executes the generated code and validates it against test cases.
    """
    local_env = {}
    try:
        # Use exec to run the generated code in a controlled environment.
        exec(generated_code, {}, local_env)

        # Check if the function we're testing actually exists in the generated code.
        if function_name not in local_env:
            return "Validation: No function definition found"

        # Get the function and run all the test cases.
        func_to_test = local_env[function_name]
        for input_data, expected_output in test_cases:
            result = func_to_test(input_data)
            if result != expected_output:
                return f"Validation: Failed test on input {input_data}. Expected {expected_output}, got {result}"

        return "Validation: All test cases passed"

    except SyntaxError as e:
        # Catch and report any syntax errors from the generated code.
        return f"Validation: Syntax error: {e}"
    except Exception as e:
        # Catch any other runtime errors.
        return f"Validation: Runtime error: {e}"


def main():
    logging.basicConfig(level=logging.INFO)

    # Example test case
    prompt = "Write a function that checks if a number is below zero"

    try:
        rag_system = RAGCodeGenerator()

        print(f"--- Testing prompt: {prompt} ---")

        # Generate the code
        result = rag_system.generate_code(prompt, top_k=3, similarity_threshold=0.1)

        print("Generated Code:")
        print(result['generated_code'])
        print(f"\nRetrieved {result['retrieval_count']} similar examples")

        # Print retrieved examples
        if result['retrieved_examples']:
            print("\nSimilar examples found:")
            for i, example in enumerate(result['retrieved_examples'][:2]):  # Show first 2
                print(f"Example {i+1} (similarity: {example['similarity']:.3f}):")
                print(example['prompt'][:200] + "..." if len(example['prompt']) > 200 else example['prompt'])
                print()

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

# Enhanced test function
def test_enhanced_generation():
    """Test the enhanced RAG system with multiple examples"""

    test_prompts = [
        "Write a function that checks if a number is below zero",
        "Create a function to check if a number is even",
        "Write a function that finds the maximum value in a list",
        "Create a function that reverses a string",
        "Write a function to check if a number is prime"
    ]

    try:
        # Use the enhanced generator
        rag_system = RAGCodeGenerator()
        # Replace the generator with our enhanced version
        rag_system.generator = EnhancedLocalCodeGenerator()

        for prompt in test_prompts:
            print(f"\n{'='*60}")
            print(f"Testing: {prompt}")
            print('='*60)

            result = rag_system.generate_code(prompt, top_k=2, similarity_threshold=0.1)

            print("Generated Code:")
            print("-" * 40)
            print(result['generated_code'])
            print("-" * 40)

            print(f"Retrieved {result['retrieval_count']} similar examples")

            if result['retrieved_examples']:
                print(f"\nTop similar example (similarity: {result['retrieved_examples'][0]['similarity']:.3f}):")
                example_preview = result['retrieved_examples'][0]['prompt'][:150]
                print(f"{example_preview}..." if len(result['retrieved_examples'][0]['prompt']) > 150 else example_preview)

            # Try to validate basic syntax
            try:
                compile(result['generated_code'], '<string>', 'exec')
                print("✅ Code compiles successfully")
            except SyntaxError as e:
                print(f"❌ Syntax error: {e}")

    except Exception as e:
        print(f"Error in enhanced testing: {e}")
        import traceback
        traceback.print_exc()


# Run the enhanced test
if __name__ == "__main__":
    test_enhanced_generation()

