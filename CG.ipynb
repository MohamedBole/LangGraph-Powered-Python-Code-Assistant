{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/tFinRb07njOMKgnTOUlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23eb1853940746bcbcf02e489a12bb1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_480a38776159452598b984e169af1a05",
              "IPY_MODEL_f403ad44a6e2477da6abbf95fddde4c2",
              "IPY_MODEL_dd84fee3e5114128b723df87939b3f37"
            ],
            "layout": "IPY_MODEL_d4a29a8339f0480190f68f98322031c8"
          }
        },
        "480a38776159452598b984e169af1a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_627ab1c3e53c41ba8bf49d61920afb89",
            "placeholder": "​",
            "style": "IPY_MODEL_773c79be18e2419a97eb57adb186747c",
            "value": "Batches: 100%"
          }
        },
        "f403ad44a6e2477da6abbf95fddde4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9413c6aec1e474da726dc1c27b8d566",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_707681f2061b4a5f9b84b3dfe1300ba3",
            "value": 6
          }
        },
        "dd84fee3e5114128b723df87939b3f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba5b8b135d340b982a27222cab305a4",
            "placeholder": "​",
            "style": "IPY_MODEL_db9582ca0de546c9a8bdfbc230c6eb25",
            "value": " 6/6 [00:03&lt;00:00,  1.96it/s]"
          }
        },
        "d4a29a8339f0480190f68f98322031c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "627ab1c3e53c41ba8bf49d61920afb89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773c79be18e2419a97eb57adb186747c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9413c6aec1e474da726dc1c27b8d566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707681f2061b4a5f9b84b3dfe1300ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ba5b8b135d340b982a27222cab305a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9582ca0de546c9a8bdfbc230c6eb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e88792b57ec74641b885ab8bcdb1dfc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1064a32bbb4f4bf2a3ef253ebf302e38",
              "IPY_MODEL_3b056cdeb5514f58b6b00615d010ebd5",
              "IPY_MODEL_e2875f5ad25f424285cea910b0e2f4a9"
            ],
            "layout": "IPY_MODEL_9b07136e190c434caff31cbb3b0f9c2b"
          }
        },
        "1064a32bbb4f4bf2a3ef253ebf302e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d9a04b16a6f40b0abf502e6d4472c62",
            "placeholder": "​",
            "style": "IPY_MODEL_1b8588c712f2414fa8e811b386cdc727",
            "value": "Batches: 100%"
          }
        },
        "3b056cdeb5514f58b6b00615d010ebd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac0b55db2e3c4340a3a64f5224906de5",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cee19d29043464c8cfc33b09c8a8c9a",
            "value": 6
          }
        },
        "e2875f5ad25f424285cea910b0e2f4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9493aa27507c4e3ca7ac5f7327d69390",
            "placeholder": "​",
            "style": "IPY_MODEL_473caaf7f7cf4db89e54b91894007da7",
            "value": " 6/6 [00:02&lt;00:00,  1.97it/s]"
          }
        },
        "9b07136e190c434caff31cbb3b0f9c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9a04b16a6f40b0abf502e6d4472c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8588c712f2414fa8e811b386cdc727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac0b55db2e3c4340a3a64f5224906de5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cee19d29043464c8cfc33b09c8a8c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9493aa27507c4e3ca7ac5f7327d69390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473caaf7f7cf4db89e54b91894007da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedBole/LangGraph-Powered-Python-Code-Assistant/blob/main/CG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Setup & Install Dependencies"
      ],
      "metadata": {
        "id": "ITEjwqMzvX9B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q-fuk9hEtYZw",
        "outputId": "9e4cda32-6158-4c9a-9805-b5c600532998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu pandas datasets torch transformers accelerate sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import ast\n",
        "import re\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n"
      ],
      "metadata": {
        "id": "Ifs-K2U5trjK"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os # Import the os module\n",
        "\n",
        "try:\n",
        "    # Replace this line with your actual API key\n",
        "    GOOGLE_API_KEY = \"AIzaSyB2_w02Fn0TzwB24Ksh6gi4e6cRqrgSLCg\"\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini: {e}\")"
      ],
      "metadata": {
        "id": "JUdfRHZ0ZSbo"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingPipeline:\n",
        "    \"\"\"Enhanced embedding generation and FAISS index creation\"\"\"\n",
        "    def __init__(self, model_name=\"all-mpnet-base-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.prompts = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    def create_embeddings(self, dataset_path=\"/content/human-eval.jsonl\"):\n",
        "        \"\"\"Load dataset and create embeddings with preprocessing\"\"\"\n",
        "        try:\n",
        "            dataset = pd.read_json(dataset_path, lines=True)\n",
        "\n",
        "            # Preprocess prompts - combine prompt and canonical_solution if available\n",
        "            self.prompts = []\n",
        "            for _, row in dataset.iterrows():\n",
        "                # Create richer context by including both prompt and solution pattern\n",
        "                prompt_text = row[\"prompt\"]\n",
        "\n",
        "                # Extract function signature and docstring for better matching\n",
        "                if \"def \" in prompt_text and '\"\"\"' in prompt_text:\n",
        "                    # Extract the docstring for semantic understanding\n",
        "                    docstring_match = re.search(r'\"\"\"(.*?)\"\"\"', prompt_text, re.DOTALL)\n",
        "                    if docstring_match:\n",
        "                        docstring = docstring_match.group(1).strip()\n",
        "                        prompt_text = f\"{prompt_text}\\nDescription: {docstring}\"\n",
        "\n",
        "                self.prompts.append(prompt_text)\n",
        "\n",
        "            # Create embeddings\n",
        "            self.embeddings = self.model.encode(self.prompts, show_progress_bar=True)\n",
        "            self.embeddings = np.array(self.embeddings).astype(\"float32\")\n",
        "\n",
        "            print(f\"Created embeddings for {len(self.prompts)} examples\")\n",
        "            return self.embeddings\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error creating embeddings: {e}\")\n",
        "            raise\n",
        "\n",
        "    def create_faiss_index(self, index_path=\"humaneval_embeddings.faiss\"):\n",
        "        \"\"\"Create and save FAISS index with better indexing strategy\"\"\"\n",
        "        if self.embeddings is None:\n",
        "            raise ValueError(\"Embeddings not created yet. Call create_embeddings first.\")\n",
        "\n",
        "        dimension = self.embeddings.shape[1]\n",
        "\n",
        "        # Use IVFFlat for better performance with larger datasets\n",
        "        if len(self.embeddings) > 1000:\n",
        "            nlist = min(100, len(self.embeddings) // 10)  # Number of clusters\n",
        "            quantizer = faiss.IndexFlatL2(dimension)\n",
        "            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
        "            index.train(self.embeddings)\n",
        "        else:\n",
        "            index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "        index.add(self.embeddings)\n",
        "        faiss.write_index(index, index_path)\n",
        "        return index"
      ],
      "metadata": {
        "id": "SA-advU9_lXg"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrievalEngine:\n",
        "    \"\"\"Enhanced similarity search with filtering and ranking\"\"\"\n",
        "    def __init__(self, index_path: str, prompts: List[str]):\n",
        "        self.index = faiss.read_index(index_path)\n",
        "        self.prompts = prompts\n",
        "        self.embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5, similarity_threshold: float = 0.3) -> List[Dict]:\n",
        "        \"\"\"Retrieve and rank similar code examples with filtering\"\"\"\n",
        "        try:\n",
        "            query_embedding = np.array(self.embedder.encode([query])).astype(\"float32\")\n",
        "\n",
        "            # Search for more candidates than needed\n",
        "            search_k = min(top_k * 3, len(self.prompts))\n",
        "            distances, indices = self.index.search(query_embedding, search_k)\n",
        "\n",
        "            # Filter and rank results\n",
        "            filtered_results = []\n",
        "            for dist, idx in zip(distances[0], indices[0]):\n",
        "                if idx != -1:  # Valid index\n",
        "                    # Convert L2 distance to similarity score (lower distance = higher similarity)\n",
        "                    similarity = 1 / (1 + dist)\n",
        "\n",
        "                    if similarity > similarity_threshold:\n",
        "                        filtered_results.append({\n",
        "                            'prompt': self.prompts[idx],\n",
        "                            'code': self.prompts[idx],  # Add code key for compatibility\n",
        "                            'similarity': similarity,\n",
        "                            'distance': dist\n",
        "                        })\n",
        "\n",
        "            # Sort by similarity and return top_k\n",
        "            filtered_results.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "            return filtered_results[:top_k]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in retrieval: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_function_info(self, code: str) -> Dict:\n",
        "        \"\"\"Extract function metadata for better matching\"\"\"\n",
        "        try:\n",
        "            # Parse the AST to get function information\n",
        "            tree = ast.parse(code)\n",
        "            for node in ast.walk(tree):\n",
        "                if isinstance(node, ast.FunctionDef):\n",
        "                    return {\n",
        "                        'name': node.name,\n",
        "                        'args': [arg.arg for arg in node.args.args],\n",
        "                        'docstring': ast.get_docstring(node) or ''\n",
        "                    }\n",
        "        except:\n",
        "            pass\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "2Sq9i-ruAc0i"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalCodeGenerator:\n",
        "    def __init__(self, model_name=\"Salesforce/codegen-350M-mono\"):\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        print(\"Model loaded successfully!\")\n",
        "\n",
        "    def generate(self, prompt: str, retrieved_examples: list) -> dict:\n",
        "        # Format the prompt as a comment, which the model is trained on\n",
        "        formatted_prompt = f\"# {prompt}\\n\"\n",
        "\n",
        "        # Add retrieved examples to the prompt if they exist\n",
        "        if retrieved_examples:\n",
        "            examples_text = \"\\n\".join([f\"# Example:\\n{ex['code']}\" for ex in retrieved_examples])\n",
        "            formatted_prompt = f\"{examples_text}\\n{formatted_prompt}\"\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        input_ids = self.tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        try:\n",
        "            # Generate new tokens\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    input_ids,\n",
        "                    max_new_tokens=256,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.95,\n",
        "                    repetition_penalty=1.1\n",
        "                )\n",
        "\n",
        "            # Decode only the newly generated part of the output\n",
        "            generated_code = self.tokenizer.decode(\n",
        "                outputs[0, input_ids.shape[-1]:],\n",
        "                skip_special_tokens=True\n",
        "            ).strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model generation: {e}\")\n",
        "            generated_code = \"\"\n",
        "\n",
        "        return {\n",
        "            'generated_code': generated_code,  # Fixed key name\n",
        "            'raw_output': generated_code,\n",
        "            'full_prompt': formatted_prompt\n",
        "        }"
      ],
      "metadata": {
        "id": "q-KJTbu4n3Oi"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedLocalCodeGenerator:\n",
        "    def __init__(self, model_name=\"Salesforce/codegen-350M-mono\"):\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        print(\"Model loaded successfully!\")\n",
        "\n",
        "    def extract_function_signature(self, prompt):\n",
        "        \"\"\"Extract function name from natural language prompt\"\"\"\n",
        "        # Simple heuristics to suggest function names\n",
        "        if \"below zero\" in prompt.lower():\n",
        "            return \"is_below_zero\"\n",
        "        elif \"positive\" in prompt.lower():\n",
        "            return \"is_positive\"\n",
        "        elif \"even\" in prompt.lower():\n",
        "            return \"is_even\"\n",
        "        elif \"prime\" in prompt.lower():\n",
        "            return \"is_prime\"\n",
        "        else:\n",
        "            return \"solution\"\n",
        "\n",
        "    def format_examples(self, retrieved_examples):\n",
        "        \"\"\"Format retrieved examples for better context\"\"\"\n",
        "        if not retrieved_examples:\n",
        "            return \"\"\n",
        "\n",
        "        examples_text = \"# Here are some similar examples:\\n\"\n",
        "        for i, example in enumerate(retrieved_examples[:2]):  # Use top 2 examples\n",
        "            # Extract just the function definition part\n",
        "            code_lines = example['code'].split('\\n')\n",
        "            func_lines = []\n",
        "            in_function = False\n",
        "\n",
        "            for line in code_lines:\n",
        "                if line.strip().startswith('def '):\n",
        "                    in_function = True\n",
        "                    func_lines.append(line)\n",
        "                elif in_function:\n",
        "                    if line.strip() == '' or line.startswith(' ') or line.startswith('\\t'):\n",
        "                        func_lines.append(line)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            if func_lines:\n",
        "                examples_text += f\"# Example {i+1}:\\n\"\n",
        "                examples_text += '\\n'.join(func_lines[:10])  # Limit lines\n",
        "                examples_text += \"\\n\\n\"\n",
        "\n",
        "        return examples_text\n",
        "\n",
        "    def generate(self, prompt: str, retrieved_examples: list) -> dict:\n",
        "        # Suggest a function name based on the prompt\n",
        "        suggested_func_name = self.extract_function_signature(prompt)\n",
        "\n",
        "        # Format examples\n",
        "        examples_context = self.format_examples(retrieved_examples)\n",
        "\n",
        "        # Create a more structured prompt\n",
        "        formatted_prompt = f\"\"\"{examples_context}# Task: {prompt}\n",
        "# Please write a Python function called '{suggested_func_name}' that solves this task:\n",
        "\n",
        "def {suggested_func_name}(\"\"\"\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        input_ids = self.tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        try:\n",
        "            # Generate new tokens with better parameters\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    input_ids,\n",
        "                    max_new_tokens=200,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.4,  # Lower temperature for more focused generation\n",
        "                    top_p=0.9,\n",
        "                    repetition_penalty=1.2,\n",
        "                    num_return_sequences=1,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            # Decode the full output\n",
        "            full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract just the generated part\n",
        "            generated_part = full_output[len(formatted_prompt):].strip()\n",
        "\n",
        "            # Complete the function definition\n",
        "            complete_function = f\"def {suggested_func_name}(\" + generated_part\n",
        "\n",
        "            # Clean up the output\n",
        "            complete_function = self.clean_generated_code(complete_function)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model generation: {e}\")\n",
        "            complete_function = f\"\"\"def {suggested_func_name}(x):\n",
        "    # Generated function for: {prompt}\n",
        "    pass\"\"\"\n",
        "\n",
        "        return {\n",
        "            'generated_code': complete_function,\n",
        "            'raw_output': generated_part if 'generated_part' in locals() else '',\n",
        "            'full_prompt': formatted_prompt,\n",
        "            'function_name': suggested_func_name\n",
        "        }\n",
        "\n",
        "    def clean_generated_code(self, code):\n",
        "        \"\"\"Clean and format the generated code\"\"\"\n",
        "        lines = code.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Stop at obvious end markers\n",
        "            if any(marker in line.lower() for marker in ['# example', '# task', '# note', 'print(']):\n",
        "                break\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "        # Remove trailing empty lines\n",
        "        while cleaned_lines and not cleaned_lines[-1].strip():\n",
        "            cleaned_lines.pop()\n",
        "\n",
        "        return '\\n'.join(cleaned_lines)"
      ],
      "metadata": {
        "id": "TbePpSrm1QIr"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGCodeGenerator:\n",
        "    \"\"\"Enhanced end-to-end RAG pipeline\"\"\"\n",
        "    def __init__(self, dataset_path: str = \"/content/human-eval.jsonl\"):\n",
        "        # Initialize components with error handling\n",
        "        try:\n",
        "            print(\"Initializing RAG Code Generator...\")\n",
        "            self.embedding_pipeline = EmbeddingPipeline()\n",
        "            self.embeddings = self.embedding_pipeline.create_embeddings(dataset_path)\n",
        "            self.index = self.embedding_pipeline.create_faiss_index()\n",
        "\n",
        "            self.retriever = RetrievalEngine(\n",
        "                \"humaneval_embeddings.faiss\",\n",
        "                self.embedding_pipeline.prompts\n",
        "            )\n",
        "\n",
        "            self.generator = EnhancedLocalCodeGenerator()\n",
        "            print(\"RAG system initialized successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error initializing RAG system: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_code(self, user_prompt: str, top_k: int = 3, similarity_threshold: float = 0.3) -> Dict:\n",
        "        \"\"\"Enhanced pipeline execution with comprehensive results\"\"\"\n",
        "        try:\n",
        "            print(f\"Generating code for prompt: {user_prompt}\")\n",
        "\n",
        "            # Retrieve similar examples\n",
        "            similar_examples = self.retriever.retrieve(\n",
        "                user_prompt,\n",
        "                top_k=top_k,\n",
        "                similarity_threshold=similarity_threshold\n",
        "            )\n",
        "\n",
        "            print(f\"Found {len(similar_examples)} similar examples\")\n",
        "\n",
        "            # Generate code\n",
        "            generation_result = self.generator.generate(user_prompt, similar_examples)\n",
        "\n",
        "            return {\n",
        "                \"prompt\": user_prompt,\n",
        "                \"retrieved_examples\": similar_examples,\n",
        "                \"generated_code\": generation_result['generated_code'],  # Fixed key access\n",
        "                \"raw_output\": generation_result['raw_output'],\n",
        "                \"retrieval_count\": len(similar_examples)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in code generation pipeline: {e}\")\n",
        "            return {\n",
        "                \"prompt\": user_prompt,\n",
        "                \"retrieved_examples\": [],\n",
        "                \"generated_code\": \"\",\n",
        "                \"raw_output\": \"\",\n",
        "                \"retrieval_count\": 0,\n",
        "                \"error\": str(e)\n",
        "            }"
      ],
      "metadata": {
        "id": "BibmCTT0BLcd"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_and_execute(generated_code, function_name, test_cases):\n",
        "    \"\"\"\n",
        "    This function safely executes the generated code and validates it against test cases.\n",
        "    \"\"\"\n",
        "    local_env = {}\n",
        "    try:\n",
        "        # Use exec to run the generated code in a controlled environment.\n",
        "        exec(generated_code, {}, local_env)\n",
        "\n",
        "        # Check if the function we're testing actually exists in the generated code.\n",
        "        if function_name not in local_env:\n",
        "            return \"Validation: No function definition found\"\n",
        "\n",
        "        # Get the function and run all the test cases.\n",
        "        func_to_test = local_env[function_name]\n",
        "        for input_data, expected_output in test_cases:\n",
        "            result = func_to_test(input_data)\n",
        "            if result != expected_output:\n",
        "                return f\"Validation: Failed test on input {input_data}. Expected {expected_output}, got {result}\"\n",
        "\n",
        "        return \"Validation: All test cases passed\"\n",
        "\n",
        "    except SyntaxError as e:\n",
        "        # Catch and report any syntax errors from the generated code.\n",
        "        return f\"Validation: Syntax error: {e}\"\n",
        "    except Exception as e:\n",
        "        # Catch any other runtime errors.\n",
        "        return f\"Validation: Runtime error: {e}\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Example test case\n",
        "    prompt = \"Write a function that checks if a number is below zero\"\n",
        "\n",
        "    try:\n",
        "        rag_system = RAGCodeGenerator()\n",
        "\n",
        "        print(f\"--- Testing prompt: {prompt} ---\")\n",
        "\n",
        "        # Generate the code\n",
        "        result = rag_system.generate_code(prompt, top_k=3, similarity_threshold=0.1)\n",
        "\n",
        "        print(\"Generated Code:\")\n",
        "        print(result['generated_code'])\n",
        "        print(f\"\\nRetrieved {result['retrieval_count']} similar examples\")\n",
        "\n",
        "        # Print retrieved examples\n",
        "        if result['retrieved_examples']:\n",
        "            print(\"\\nSimilar examples found:\")\n",
        "            for i, example in enumerate(result['retrieved_examples'][:2]):  # Show first 2\n",
        "                print(f\"Example {i+1} (similarity: {example['similarity']:.3f}):\")\n",
        "                print(example['prompt'][:200] + \"...\" if len(example['prompt']) > 200 else example['prompt'])\n",
        "                print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "23eb1853940746bcbcf02e489a12bb1b",
            "480a38776159452598b984e169af1a05",
            "f403ad44a6e2477da6abbf95fddde4c2",
            "dd84fee3e5114128b723df87939b3f37",
            "d4a29a8339f0480190f68f98322031c8",
            "627ab1c3e53c41ba8bf49d61920afb89",
            "773c79be18e2419a97eb57adb186747c",
            "e9413c6aec1e474da726dc1c27b8d566",
            "707681f2061b4a5f9b84b3dfe1300ba3",
            "3ba5b8b135d340b982a27222cab305a4",
            "db9582ca0de546c9a8bdfbc230c6eb25"
          ]
        },
        "id": "C2Rjm9GCBan3",
        "outputId": "c4aa4cc8-8053-4600-cd07-92ec53bf3eb3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing RAG Code Generator...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23eb1853940746bcbcf02e489a12bb1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created embeddings for 164 examples\n",
            "Loading model: Salesforce/codegen-350M-mono\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "RAG system initialized successfully!\n",
            "--- Testing prompt: Write a function that checks if a number is below zero ---\n",
            "Generating code for prompt: Write a function that checks if a number is below zero\n",
            "Found 3 similar examples\n",
            "Generated Code:\n",
            "def check_below_zero():\n",
            "    print('check below zero')\n",
            "\n",
            "check_below_zero()\n",
            "\n",
            "Retrieved 3 similar examples\n",
            "\n",
            "Similar examples found:\n",
            "Example 1 (similarity: 0.581):\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task...\n",
            "\n",
            "Example 2 (similarity: 0.522):\n",
            "\n",
            "\n",
            "def below_threshold(l: list, t: int):\n",
            "    \"\"\"Return True if all numbers in the list l are below threshold t.\n",
            "    >>> below_threshold([1, 2, 4, 10], 100)\n",
            "    True\n",
            "    >>> below_threshold([1, 20, 4, 1...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced test function\n",
        "def test_enhanced_generation():\n",
        "    \"\"\"Test the enhanced RAG system with multiple examples\"\"\"\n",
        "\n",
        "    test_prompts = [\n",
        "        \"Write a function that checks if a number is below zero\",\n",
        "        \"Create a function to check if a number is even\",\n",
        "        \"Write a function that finds the maximum value in a list\",\n",
        "        \"Create a function that reverses a string\",\n",
        "        \"Write a function to check if a number is prime\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Use the enhanced generator\n",
        "        rag_system = RAGCodeGenerator()\n",
        "        # Replace the generator with our enhanced version\n",
        "        rag_system.generator = EnhancedLocalCodeGenerator()\n",
        "\n",
        "        for prompt in test_prompts:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Testing: {prompt}\")\n",
        "            print('='*60)\n",
        "\n",
        "            result = rag_system.generate_code(prompt, top_k=2, similarity_threshold=0.1)\n",
        "\n",
        "            print(\"Generated Code:\")\n",
        "            print(\"-\" * 40)\n",
        "            print(result['generated_code'])\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            print(f\"Retrieved {result['retrieval_count']} similar examples\")\n",
        "\n",
        "            if result['retrieved_examples']:\n",
        "                print(f\"\\nTop similar example (similarity: {result['retrieved_examples'][0]['similarity']:.3f}):\")\n",
        "                example_preview = result['retrieved_examples'][0]['prompt'][:150]\n",
        "                print(f\"{example_preview}...\" if len(result['retrieved_examples'][0]['prompt']) > 150 else example_preview)\n",
        "\n",
        "            # Try to validate basic syntax\n",
        "            try:\n",
        "                compile(result['generated_code'], '<string>', 'exec')\n",
        "                print(\"✅ Code compiles successfully\")\n",
        "            except SyntaxError as e:\n",
        "                print(f\"❌ Syntax error: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in enhanced testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "# Run the enhanced test\n",
        "if __name__ == \"__main__\":\n",
        "    test_enhanced_generation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e88792b57ec74641b885ab8bcdb1dfc1",
            "1064a32bbb4f4bf2a3ef253ebf302e38",
            "3b056cdeb5514f58b6b00615d010ebd5",
            "e2875f5ad25f424285cea910b0e2f4a9",
            "9b07136e190c434caff31cbb3b0f9c2b",
            "8d9a04b16a6f40b0abf502e6d4472c62",
            "1b8588c712f2414fa8e811b386cdc727",
            "ac0b55db2e3c4340a3a64f5224906de5",
            "4cee19d29043464c8cfc33b09c8a8c9a",
            "9493aa27507c4e3ca7ac5f7327d69390",
            "473caaf7f7cf4db89e54b91894007da7"
          ]
        },
        "id": "ebFpebMiv1KE",
        "outputId": "02404ad3-ac88-4974-99cb-6c2218851cad"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing RAG Code Generator...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e88792b57ec74641b885ab8bcdb1dfc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created embeddings for 164 examples\n",
            "Loading model: Salesforce/codegen-350M-mono\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "RAG system initialized successfully!\n",
            "Loading model: Salesforce/codegen-350M-mono\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n",
            "============================================================\n",
            "Testing: Write a function that checks if a number is below zero\n",
            "============================================================\n",
            "Generating code for prompt: Write a function that checks if a number is below zero\n",
            "Found 2 similar examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "----------------------------------------\n",
            "def is_below_zero(num: int) -> bool:  # noqa E741\n",
            "    if num < 0 or (not type == \"integer\"):\n",
            "        return False\n",
            "    else:\n",
            "        for i in range(-2 ** 31 + 1, 2 * (-2 ** 31), -1):\n",
            "            a = str((i).bit_length())\n",
            "            b = len('0x')\n",
            "            c = b // 8\n",
            "            d = c % 16\n",
            "            e = d / 32\n",
            "            f = e//8*32+64-16 # 64 bits for integer values between two bytes\n",
            "            g= f%256\n",
            "            h=(str)(hex(f))\n",
            "            j=\"0x\".join(list(\"{}{}\".format(*map(lambda x : hex(ord(chr(97+(k)))), h))))\n",
            "            print (\"{}: {}:{}\".format(len(h)+3 , j, g ))\n",
            "----------------------------------------\n",
            "Retrieved 2 similar examples\n",
            "\n",
            "Top similar example (similarity: 0.581):\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank ac...\n",
            "✅ Code compiles successfully\n",
            "\n",
            "============================================================\n",
            "Testing: Create a function to check if a number is even\n",
            "============================================================\n",
            "Generating code for prompt: Create a function to check if a number is even\n",
            "Found 2 similar examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "----------------------------------------\n",
            "def is_even(number):\n",
            "    if int(str(number)[0]) %2== 0 : # Checks for all non-zero integers in string form; returns true or false depending on condition being met\n",
            "       return \"true\"  # If so then it will print out boolean value indicating which variable was used when checking conditions\n",
            "    else: \n",
            "----------------------------------------\n",
            "Retrieved 2 similar examples\n",
            "\n",
            "Top similar example (similarity: 0.626):\n",
            "\n",
            "def is_equal_to_sum_even(n):\n",
            "    \"\"\"Evaluate whether the given number n can be written as the sum of exactly 4 positive even numbers\n",
            "    Example\n",
            "    ...\n",
            "❌ Syntax error: expected an indented block after 'else' statement on line 4 (<string>, line 4)\n",
            "\n",
            "============================================================\n",
            "Testing: Write a function that finds the maximum value in a list\n",
            "============================================================\n",
            "Generating code for prompt: Write a function that finds the maximum value in a list\n",
            "Found 2 similar examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "----------------------------------------\n",
            "def solution(a) -> int:\n",
            "  max=0; i=-2 # initial index is negative because we want to find last item as it's largest number so need to subtract one from current index (i-1).\n",
            "  for j in range(len(a)-1,-1,-1): # iterate through all elements except for last element which has already been checked by previous iteration\n",
            "      if not a[j]: continue # if there is no more than two items left then skip till next non empty item\n",
            "      else : break # if there exists at least one other item or none remaining then stop\n",
            "      k=(int)(((float)(sum)/j))+1 # sum up all values divided by num of times they were used but multiplied by their respective position within the loop\n",
            "----------------------------------------\n",
            "Retrieved 2 similar examples\n",
            "\n",
            "Top similar example (similarity: 0.653):\n",
            "\n",
            "\n",
            "def max_element(l: list):\n",
            "    \"\"\"Return maximum element in the list.\n",
            "    >>> max_element([1, 2, 3])\n",
            "    3\n",
            "    >>> max_element([5, 3, -5, 2, -3, 3, 9...\n",
            "✅ Code compiles successfully\n",
            "\n",
            "============================================================\n",
            "Testing: Create a function that reverses a string\n",
            "============================================================\n",
            "Generating code for prompt: Create a function that reverses a string\n",
            "Found 2 similar examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "----------------------------------------\n",
            "def solution(string1, string2):\n",
            "\n",
            "    return str(''.join([char*(-len(list(str(x)))+1)+chr((ord(y)-97))for y in list(reversed(string1)]).replace('-','.')).upper()) + \\\n",
            "           ''.join([char * (- len (list(str(z)))) + chr (( ord(l) - 97 ) % 26) for z in re.findall('\\d{3}\\.?\\w+|\\D+', string2)].lower() ).strip().capitalize(), False\n",
            "----------------------------------------\n",
            "Retrieved 2 similar examples\n",
            "\n",
            "Top similar example (similarity: 0.590):\n",
            "\n",
            "def solve(s):\n",
            "    \"\"\"You are given a string s.\n",
            "    if s[i] is a letter, reverse its case from lower to upper or vise versa, \n",
            "    otherwise keep it as...\n",
            "❌ Syntax error: closing parenthesis ']' does not match opening parenthesis '(' (<string>, line 3)\n",
            "\n",
            "============================================================\n",
            "Testing: Write a function to check if a number is prime\n",
            "============================================================\n",
            "Generating code for prompt: Write a function to check if a number is prime\n",
            "Found 2 similar examples\n",
            "Generated Code:\n",
            "----------------------------------------\n",
            "def is_prime(num):\n",
            "    if num <= 0 or not any([i % 2 for i in range(2, int(math.sqrt(num)) + 1)]):  return False\n",
            "    for n in [0] + list(range(-int((1 << math.log10(x)).bit_length()) // 2 - 1, len(str(bin(num))))[::-1]: # Use bit length here because we want to find out how many digits after each base can be represented as an integer with no leading zeroes!\n",
            "----------------------------------------\n",
            "Retrieved 2 similar examples\n",
            "\n",
            "Top similar example (similarity: 0.700):\n",
            "\n",
            "\n",
            "def is_prime(n):\n",
            "    \"\"\"Return true if a given number is prime, and false otherwise.\n",
            "    >>> is_prime(6)\n",
            "    False\n",
            "    >>> is_prime(101)\n",
            "    True\n",
            "  ...\n",
            "❌ Syntax error: invalid syntax (<string>, line 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP_oc4ms1qdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}